# Technical-Assessment-Software-Craftsperson-21-10-2024

## Project Overview
This project demonstrates an ETL (Extract, Transform, Load) pipeline implementation using **PySpark**, **SQL**, **Azure Data Lake Storage (ADLS)**, and **Databricks**. The pipeline handles data ingestion, transformation, and loading processes, ensuring efficient data processing and storage.

## Key Technologies Used
- **PySpark**: For distributed data processing.
- **SQL**: For querying and managing structured data.
- **Azure Data Lake Storage (ADLS)**: For scalable data storage.
- **Databricks**: For executing PySpark jobs and managing the ETL process.

## Features Implemented
1. **Data Ingestion**: 
   - Loaded raw data from ADLS into the pipeline.
   
2. **Data Transformation**: 
   - Cleaned and transformed the data using PySpark functions.
   - Applied necessary business logic to ensure data quality and accuracy.

3. **Data Loading**: 
   - Loaded the processed data into target tables in the desired format.
   - Split data into country-specific tables for better organization.

4. **Data Inspection**:
   - Displayed records of the transformed data for verification using `df.display()`.

## Contact
For any questions or further discussion about this assignment, please reach out to:

**Abhishek Rajendra Mandge**  
Email: abhishekmandge@gmail.com  
LinkedIn: [Abhishek Mandge](https://www.linkedin.com/in/abhishekmandge)  
GitHub: [AbhishekMandge](https://github.com/AbhishekMandge)

